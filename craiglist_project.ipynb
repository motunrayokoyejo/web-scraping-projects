{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key': 'value'}\n",
      "{'key': 'value', 'new key': 'new value'}\n"
     ]
    }
   ],
   "source": [
    "url = \"https://boston.craigslist.org/search/sof\"\n",
    "\n",
    "#creates a dictionary\n",
    "d = {'key':'value'}\n",
    "print(d)\n",
    "\n",
    "#update dictionary\n",
    "d['new key'] = 'new value'\n",
    "print(d)\n",
    "npo_jobs = {}\n",
    "\n",
    "job_no = 0\n",
    "while True:\n",
    "\n",
    "    response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = response.text\n",
    "soup = BeautifulSoup(data,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = soup.find_all('a',{'class':'result-title'})\n",
    "for title in titles:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = soup.find_all('span',{'class':'result-hood'})\n",
    "for address in addresses:\n",
    "        print(address.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = soup.find_all('p',{'class':'result-info'})\n",
    "for job in jobs:\n",
    "    title = job.find('a',{'class':'result-title'}).text\n",
    "    location_tag = job.find('span',{'class':'result-hood'})\n",
    "    location = location_tag.text[2:-1] if location_tag else \"N/A\"\n",
    "    date = job.find('time',{'class':'result-date'}).text\n",
    "    link = job.find('a',{'class':'result-title'}).get('href')\n",
    "    job_response = requests.get(link)\n",
    "    job_data = job_response.text\n",
    "    job_soup = BeautifulSoup(job_data, 'html.parser')\n",
    "    job_description = job_soup.find('section',{'id':'postingbody'}).text\n",
    "    job_attributes_tag = job_soup.find('p',{'class':'attrgroup'})\n",
    "    job_attributes = job_attributes_tag.text if job_attributes_tag else \"N/A\"\n",
    "    job_no +=1\n",
    "    npo_jobs[job_no] = [title,location,date,link,job_description,job_attributes]\n",
    "   # print('Job Title:', title, '\\nLocation', location, '\\nDate:', date, '\\nLink:', link, '\\Job Description:', job_description, 'Job Attributes:', job_attributes,'/n---')\n",
    "    url_tag = soup.find('a{'title': 'next page'}')\n",
    "    if url_tag.get('href'):\n",
    "       url = 'https://boston.craigslist.org' + url_tag.get('href')\n",
    "        print(url)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "print(\"Total Jobs:\", job_no)\n",
    "\n",
    "npo_jobs_df = pd.DataFrame.from_dict(npo_jobs, orient='index', columns= ['Job Title','Job Location','Date','Link', 'Descripton','Attributes'])\n",
    "print(npo_jobs_df.head())\n",
    "npo_jobs_df.to_csv('npo_jobs.csv')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
